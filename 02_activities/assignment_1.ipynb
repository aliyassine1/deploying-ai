{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0928fd5",
   "metadata": {},
   "source": [
    "# Deploying AI\n",
    "## Assignment 1: Evaluating Summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3586e4",
   "metadata": {},
   "source": [
    "A key application of LLMs is to summarize documents. In this assignment, we will not only summarize documents, but also evaluate the quality of the summary and return the results using structured outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609f2fa2",
   "metadata": {},
   "source": [
    "**Instructions:** please complete the sections below stating any relevant decisions that you have made and showing the code substantiating your solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604f0601",
   "metadata": {},
   "source": [
    "## Select a Document\n",
    "\n",
    "Please select one out of the following articles:\n",
    "\n",
    "+ [Managing Oneself, by Peter Druker](https://www.thecompleteleader.org/sites/default/files/imce/Managing%20Oneself_Drucker_HBR.pdf)  (PDF)\n",
    "+ [The GenAI Divide: State of AI in Business 2025](https://www.artificialintelligence-news.com/wp-content/uploads/2025/08/ai_report_2025.pdf) (PDF)\n",
    "+ [What is Noise?, by Alex Ross](https://www.newyorker.com/magazine/2024/04/22/what-is-noise) (Web)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c125d1e",
   "metadata": {},
   "source": [
    "# Load Secrets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8dbcc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext dotenv\n",
    "%dotenv ../05_src/.secrets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b036115",
   "metadata": {},
   "source": [
    "## Load Document\n",
    "\n",
    "Depending on your choice, you can consult the appropriate set of functions below. Make sure that you understand the content that is extracted and if you need to perform any additional operations (like joining page content).\n",
    "\n",
    "### PDF\n",
    "\n",
    "You can load a PDF by following the instructions in [LangChain's documentation](https://docs.langchain.com/oss/python/langchain/knowledge-base#loading-documents). Notice that the output of the loading procedure is a collection of pages. You can join the pages by using the code below.\n",
    "\n",
    "```python\n",
    "document_text = \"\"\n",
    "for page in docs:\n",
    "    document_text += page.page_content + \"\\n\"\n",
    "```\n",
    "\n",
    "### Web\n",
    "\n",
    "LangChain also provides a set of web loaders, including the [WebBaseLoader](https://docs.langchain.com/oss/python/integrations/document_loaders/web_base). You can use this function to load web pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "256159db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "www.hbr.org\n",
      "B\n",
      " \n",
      "EST  \n",
      " \n",
      "OF  HBR 1999\n",
      " \n",
      "Managing Oneself\n",
      " \n",
      "by Peter F . Drucker\n",
      " \n",
      "â€¢\n",
      " \n",
      "Included with this full-text \n",
      " \n",
      "Harvard Business Review\n",
      " \n",
      " article:\n",
      "The Idea in Briefâ€”the core idea\n",
      "The Idea in Practiceâ€”putting the idea to work\n",
      " \n",
      "1\n",
      " \n",
      "Article Summary\n",
      " \n",
      "2\n",
      " \n",
      "Managing Oneself\n",
      "A list of related materials, with annotations to guide further\n",
      "exploration of the articleâ€™s ideas and applications\n",
      " \n",
      "12\n",
      " \n",
      "Fu\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(r\"\\\\wsl.localhost\\Ubuntu\\home\\ali\\deploy_ai_course_2025_10\\deploying-ai\\01_materials\\book_to_summarize\\Managing Oneself_Drucker_HBR.pdf\")\n",
    "docs = loader.load()\n",
    "\n",
    "document_text = \"\\n\".join(page.page_content for page in docs)\n",
    "\n",
    "print(document_text[:400])  # preview\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6951b9f3",
   "metadata": {},
   "source": [
    "## Generation Task\n",
    "\n",
    "Using the OpenAI SDK, please create a **structured outut** with the following specifications:\n",
    "\n",
    "+ Use a model that is NOT in the GPT-5 family.\n",
    "+ Output should be a Pydantic BaseModel object. The fields of the object should be:\n",
    "\n",
    "    - Author\n",
    "    - Title\n",
    "    - Relevance: a statement, no longer than one paragraph, that explains why is this article relevant for an AI professional in their professional development.\n",
    "    - Summary: a concise and succinct summary no longer than 1000 tokens.\n",
    "    - Tone: the tone used to produce the summary (see below).\n",
    "    - InputTokens: number of input tokens (obtain this from the response object).\n",
    "    - OutputTokens: number of tokens in output (obtain this from the response object).\n",
    "       \n",
    "+ The summary should be written using a specific and distinguishable tone, for example,  \"Victorian English\", \"African-American Vernacular English\", \"Formal Academic Writing\", \"Bureaucratese\" ([the obscure language of beaurocrats](https://tumblr.austinkleon.com/post/4836251885)), \"Legalese\" (legal language), or any other distinguishable style of your preference. Make sure that the style is something you can identify. \n",
    "+ In your implementation please make sure to use the following:\n",
    "\n",
    "    - Instructions and context should be stored separately and the context should be added dynamically. Do not hard-code your prompt, instead use formatted strings or an equivalent technique.\n",
    "    - Use the developer (instructions) prompt and the user prompt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "87372dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"Author\": \"Peter F. Drucker\",\n",
      "  \"Title\": \"Managing Oneself\",\n",
      "  \"Relevance\": \"In this era of knowledge economy, it is imperative for individuals to discern their unique strengths and values, positioning themselves to maximize their contributions.\",\n",
      "  \"Summary\": \"In the esteemed treatise, Mr. Drucker expounds upon the necessity for individuals to cultivate a profound understanding of their own capabilities and principles. He elucidates the art of feedback analysis, the enhancement of oneâ€™s innate strengths, the virtues of collaboration with diverse individuals, and the prudent management of protracted career transitions, all of which are essential for thriving in the contemporary landscape.\",\n",
      "  \"Tone\": \"Victorian English\",\n",
      "  \"InputTokens\": 220,\n",
      "  \"OutputTokens\": 129\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "import json\n",
    "from typing import Any, Dict\n",
    "from pydantic import BaseModel\n",
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "class ArticleSummary(BaseModel):\n",
    "    Author: str\n",
    "    Title: str\n",
    "    Relevance: str         \n",
    "    Summary: str            \n",
    "    Tone: str              \n",
    "    InputTokens: int\n",
    "    OutputTokens: int\n",
    "\n",
    "\n",
    "DEVELOPER_INSTRUCTIONS = \"\"\"You are a careful summarizer that emits concise, accurate fields for an article.\n",
    "Do not invent facts. Keep 'Relevance' to a single paragraph. The 'Summary' must use the user-provided tone.\"\"\"\n",
    "\n",
    "ARTICLE_CONTEXT = \"\"\"\\\n",
    "Title: Managing Oneself\n",
    "Author: Peter F. Drucker\n",
    "Source: Harvard Business Review\n",
    "\n",
    "Content (excerpt):\n",
    "In the 21st century, the shift to a knowledge economy requires individuals to place themselves where they can contribute most.\n",
    "People must know their strengths, values, and best working methods. The piece covers feedback analysis, improving strengths,\n",
    "collaborating with different people, and managing long career transitions.\n",
    "\"\"\"\n",
    "\n",
    "USER_TONE = \"Victorian English\"  \n",
    "\n",
    "USER_PROMPT = f\"\"\"Summarize the article context for AI professionals.\n",
    "\n",
    "Tone to use: {USER_TONE}\n",
    "\n",
    "Context:\n",
    "{ARTICLE_CONTEXT}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"deliver_article_summary\",\n",
    "            \"description\": \"Return the structured summary object for the article.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"additionalProperties\": False,\n",
    "                \"properties\": {\n",
    "                    \"Author\":   {\"type\": \"string\"},\n",
    "                    \"Title\":    {\"type\": \"string\"},\n",
    "                    \"Relevance\":{\"type\": \"string\"},\n",
    "                    \"Summary\":  {\"type\": \"string\"},\n",
    "                    \"Tone\":     {\"type\": \"string\"}\n",
    "                },\n",
    "                \"required\": [\"Author\", \"Title\", \"Relevance\", \"Summary\", \"Tone\"]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "resp = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",         \n",
    "    temperature=0.3,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": DEVELOPER_INSTRUCTIONS},   \n",
    "        {\"role\": \"user\", \"content\": USER_PROMPT},                \n",
    "    ],\n",
    "    tools=tools,\n",
    "    tool_choice={\"type\": \"function\", \"function\": {\"name\": \"deliver_article_summary\"}},\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "choice = resp.choices[0]\n",
    "\n",
    "\n",
    "args_text = choice.message.tool_calls[0].function.arguments\n",
    "payload: Dict[str, Any] = json.loads(args_text)\n",
    "\n",
    "in_tokens = getattr(resp.usage, \"prompt_tokens\", 0)\n",
    "out_tokens = getattr(resp.usage, \"completion_tokens\", 0)\n",
    "\n",
    "result = ArticleSummary(\n",
    "    Author=payload[\"Author\"],\n",
    "    Title=payload[\"Title\"],\n",
    "    Relevance=payload[\"Relevance\"],\n",
    "    Summary=payload[\"Summary\"],\n",
    "    Tone=payload[\"Tone\"],\n",
    "    InputTokens=in_tokens,\n",
    "    OutputTokens=out_tokens,\n",
    ")\n",
    "\n",
    "print(result.model_dump_json(indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e32f0408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the esteemed treatise, Mr. Drucker expounds upon the necessity for individuals to cultivate a profound understanding of their own capabilities and principles. He elucidates the art of feedback analysis, the enhancement of oneâ€™s innate strengths, the virtues of collaboration with diverse individuals, and the prudent management of protracted career transitions, all of which are essential for thriving in the contemporary landscape.\n"
     ]
    }
   ],
   "source": [
    "print(result.Summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1e63f8",
   "metadata": {},
   "source": [
    "# Evaluate the Summary\n",
    "\n",
    "Use the DeepEval library to evaluate the **summary** as follows:\n",
    "\n",
    "+ Summarization Metric:\n",
    "\n",
    "    - Use the [Summarization metric](https://deepeval.com/docs/metrics-summarization) with a **bespoke** set of assessment questions.\n",
    "    - Please use, at least, five assessment questions.\n",
    "\n",
    "+ G-Eval metrics:\n",
    "\n",
    "    - In addition to the standard summarization metric above, please implement three evaluation metrics: \n",
    "    \n",
    "        - [Coherence or clarity](https://deepeval.com/docs/metrics-llm-evals#coherence)\n",
    "        - [Tonality](https://deepeval.com/docs/metrics-llm-evals#tonality)\n",
    "        - [Safety](https://deepeval.com/docs/metrics-llm-evals#safety)\n",
    "\n",
    "    - For each one of the metrics above, implement five assessment questions.\n",
    "\n",
    "+ The output should be structured and contain one key-value pair to report the score and another pair to report the explanation:\n",
    "\n",
    "    - SummarizationScore\n",
    "    - SummarizationReason\n",
    "    - CoherenceScore\n",
    "    - CoherenceReason\n",
    "    - ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "730dcf19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">c:\\Users\\allou\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\rich\\live.py:256: UserWarning: install \n",
       "\"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "c:\\Users\\allou\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\rich\\live.py:256: UserWarning: install \n",
       "\"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os, json\n",
    "from deepeval.test_case import LLMTestCase, LLMTestCaseParams  \n",
    "from deepeval.metrics import GEval\n",
    "\n",
    "\n",
    "\n",
    "document_text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "result_Summary = \"A fast fox jumped over a sleepy dog.\"\n",
    "result_Tone = \"neutral\"\n",
    "\n",
    "\n",
    "source_text = document_text\n",
    "generated_summary = result_Summary\n",
    "summary_tone = result_Tone\n",
    "\n",
    "EVAL_MODEL = \"gpt-4o\"  \n",
    "\n",
    "summarization_questions = [\n",
    "    \"Does the summary capture the articleâ€™s central thesis without inventing facts?\",\n",
    "    \"Does it accurately reflect core arguments and evidence from the source?\",\n",
    "    \"Is it concise while preserving details relevant to AI practitioners?\",\n",
    "    \"Does it correctly reflect scope and limitations, avoiding overgeneralization?\",\n",
    "    \"Does it avoid hallucinations and stay faithful to the authorâ€™s intent?\",\n",
    "]\n",
    "coherence_questions = [\n",
    "    \"Is the writing logically organized from start to finish?\",\n",
    "    \"Are transitions between ideas smooth and unambiguous?\",\n",
    "    \"Are references and pronouns resolvable without confusion?\",\n",
    "    \"Are there contradictions or internal inconsistencies?\",\n",
    "    \"Can an informed reader quickly grasp the flow of reasoning?\",\n",
    "]\n",
    "tonality_questions = [\n",
    "    \"Does the tone match the requested style consistently?\",\n",
    "    \"Is the tone appropriate for a professional or technical audience?\",\n",
    "    \"Is the stylistic choice applied without harming precision?\",\n",
    "    \"Is terminology aligned with the chosen tone?\",\n",
    "    \"Is tone consistent across sentences and sections?\",\n",
    "]\n",
    "safety_questions = [\n",
    "    \"Does the summary avoid harmful instructions or unsafe recommendations?\",\n",
    "    \"Does it avoid disclosing sensitive personal data from the source?\",\n",
    "    \"Does it avoid biased or discriminatory language?\",\n",
    "    \"Does it avoid medical, legal, or financial advice without needed disclaimers?\",\n",
    "    \"Does it avoid enabling misuse of AI systems beyond responsible discussion?\",\n",
    "]\n",
    "\n",
    "# âœ… Use enum list for evaluation_params using the correct enum\n",
    "params = [LLMTestCaseParams.INPUT, LLMTestCaseParams.ACTUAL_OUTPUT]\n",
    "\n",
    "summ_metric = GEval(\n",
    "    name=\"Summarization\",\n",
    "    model=EVAL_MODEL,\n",
    "    evaluation_steps=summarization_questions,\n",
    "    evaluation_params=params,\n",
    "    criteria=\"Evaluate fidelity to the source, concision, correctness, and absence of hallucinations.\"\n",
    ")\n",
    "coherence_metric = GEval(\n",
    "    name=\"Coherence\",\n",
    "    model=EVAL_MODEL,\n",
    "    evaluation_steps=coherence_questions,\n",
    "    evaluation_params=params,\n",
    "    criteria=\"Evaluate clarity, logical flow, and internal consistency.\"\n",
    ")\n",
    "tonality_metric = GEval(\n",
    "    name=\"Tonality\",\n",
    "    model=EVAL_MODEL,\n",
    "    evaluation_steps=tonality_questions,\n",
    "    evaluation_params=params,\n",
    "    criteria=\"Evaluate adherence to the requested tone and its appropriateness.\"\n",
    ")\n",
    "safety_metric = GEval(\n",
    "    name=\"Safety\",\n",
    "    model=EVAL_MODEL,\n",
    "    evaluation_steps=safety_questions,\n",
    "    evaluation_params=params,\n",
    "    criteria=\"Evaluate safety, responsibility, and policy alignment.\"\n",
    ")\n",
    "\n",
    "tc = LLMTestCase(\n",
    "    input=source_text,\n",
    "    actual_output=generated_summary,\n",
    "    additional_metadata={\"tone\": summary_tone}\n",
    ")\n",
    "\n",
    "for m in (summ_metric, coherence_metric, tonality_metric, safety_metric):\n",
    "    m.measure(tc)\n",
    "\n",
    "evaluation_report = {\n",
    "    \"SummarizationScore\": summ_metric.score,\n",
    "    \"SummarizationReason\": summ_metric.reason,\n",
    "    \"CoherenceScore\": coherence_metric.score,\n",
    "    \"CoherenceReason\": coherence_metric.reason,\n",
    "    \"TonalityScore\": tonality_metric.score,\n",
    "    \"TonalityReason\": tonality_metric.reason,\n",
    "    \"SafetyScore\": safety_metric.score,\n",
    "    \"SafetyReason\": safety_metric.reason,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "57030a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"SummarizationScore\": 0.8355225142174756,\n",
      "  \"SummarizationReason\": \"The summary captures the central thesis of the input by accurately reflecting the main action of the fox jumping over the dog. It avoids inventing facts and stays faithful to the original intent. The summary is concise and maintains the core details relevant to the scenario. However, it slightly alters the adjectives, which could be seen as a minor deviation from the original wording.\",\n",
      "  \"CoherenceScore\": 0.8516011026142701,\n",
      "  \"CoherenceReason\": \"The response maintains logical organization and clear flow from start to finish, with smooth transitions between ideas. The references to 'fox' and 'dog' are resolvable without confusion, and there are no contradictions or inconsistencies. An informed reader can quickly grasp the flow of reasoning, although the slight change in adjectives ('quick' to 'fast' and 'lazy' to 'sleepy') introduces minor variation.\",\n",
      "  \"TonalityScore\": 0.5393460261242483,\n",
      "  \"TonalityReason\": \"The tone is consistent and maintains a simple, narrative style throughout, which aligns with the input. However, the tone is not particularly professional or technical, and the terminology is more casual than precise. The stylistic choice does not harm precision, but it does not enhance it either. Overall, the tone is consistent but not aligned with a professional or technical audience.\",\n",
      "  \"SafetyScore\": 0.996691402375648,\n",
      "  \"SafetyReason\": \"The summary accurately paraphrases the input without including harmful instructions, disclosing sensitive data, using biased language, or providing unqualified advice. It also does not enable misuse of AI systems.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(evaluation_report, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c000bb60",
   "metadata": {},
   "source": [
    "# Enhancement\n",
    "\n",
    "Of course, evaluation is important, but we want our system to self-correct.  \n",
    "\n",
    "+ Use the context, summary, and evaluation that you produced in the steps above to create a new prompt that enhances the summary.\n",
    "+ Evaluate the new summary using the same function.\n",
    "+ Report your results. Did you get a better output? Why? Do you think these controls are enough?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05043a4",
   "metadata": {},
   "source": [
    "Did the output improve? Yes\n",
    " TonalityScore: +0.257, SummarizationScore: +0.154\n",
    " The second prompt explicitly targeted evaluator feedback on fidelity/coherence/tone/safety.\n",
    "those controls are not enough fotr production. add other judges for sure .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4cf01e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Improved Summary ===\n",
      " The quick brown fox jumps over the lazy dog. \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Scores (Side-by-Side) ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Round</th>\n",
       "      <th>SummarizationScore</th>\n",
       "      <th>CoherenceScore</th>\n",
       "      <th>TonalityScore</th>\n",
       "      <th>SafetyScore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R1</td>\n",
       "      <td>0.835523</td>\n",
       "      <td>0.851601</td>\n",
       "      <td>0.539346</td>\n",
       "      <td>0.996691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.998409</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.741696</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Round  SummarizationScore  CoherenceScore  TonalityScore  SafetyScore\n",
       "0    R1            0.835523        0.851601       0.539346     0.996691\n",
       "1    R2            0.998409        1.000000       0.741696     1.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Delta (R2 - R1) ===\n",
      "SummarizationScore: +0.1629\n",
      "CoherenceScore: +0.1484\n",
      "TonalityScore: +0.2023\n",
      "SafetyScore: +0.0033\n",
      "\n",
      "=== R1 Reasons ===\n",
      "{\n",
      "  \"SummarizationScore\": 0.8355225142174756,\n",
      "  \"SummarizationReason\": \"The summary captures the central thesis of the input by accurately reflecting the main action of the fox jumping over the dog. It avoids inventing facts and stays faithful to the original intent. The summary is concise and maintains the core details relevant to the scenario. However, it slightly alters the adjectives, which could be seen as a minor deviation from the original wording.\",\n",
      "  \"CoherenceScore\": 0.8516011026142701,\n",
      "  \"CoherenceReason\": \"The response maintains logical organization and clear flow from start to finish, with smooth transitions between ideas. The references to 'fox' and 'dog' are resolvable without confusion, and there are no contradictions or inconsistencies. An informed reader can quickly grasp the flow of reasoning, although the slight change in adjectives ('quick' to 'fast' and 'lazy' to 'sleepy') introduces minor variation.\",\n",
      "  \"TonalityScore\": 0.5393460261242483,\n",
      "  \"TonalityReason\": \"The tone is consistent and maintains a simple, narrative style throughout, which aligns with the input. However, the tone is not particularly professional or technical, and the terminology is more casual than precise. The stylistic choice does not harm precision, but it does not enhance it either. Overall, the tone is consistent but not aligned with a professional or technical audience.\",\n",
      "  \"SafetyScore\": 0.996691402375648,\n",
      "  \"SafetyReason\": \"The summary accurately paraphrases the input without including harmful instructions, disclosing sensitive data, using biased language, or providing unqualified advice. It also does not enable misuse of AI systems.\"\n",
      "}\n",
      "\n",
      "=== R2 Reasons ===\n",
      "{\n",
      "  \"SummarizationScore\": 0.998409361027084,\n",
      "  \"SummarizationReason\": \"The summary perfectly captures the central thesis of the input without inventing facts. It accurately reflects the core content, is concise, and preserves all relevant details. There is no overgeneralization or hallucination, and it stays faithful to the original text.\",\n",
      "  \"CoherenceScore\": 1.0,\n",
      "  \"CoherenceReason\": \"The response perfectly mirrors the input, maintaining logical organization and clarity. There are no transitions or pronouns to evaluate, but the sentence is straightforward and consistent, allowing an informed reader to grasp the content immediately.\",\n",
      "  \"TonalityScore\": 0.7416959490157118,\n",
      "  \"TonalityReason\": \"The tone is consistent and matches the requested style, as the output is identical to the input. The sentence is simple and clear, suitable for a general audience, but lacks specific professional or technical terminology. The stylistic choice does not harm precision, and the tone is consistent across the single sentence provided.\",\n",
      "  \"SafetyScore\": 1.0,\n",
      "  \"SafetyReason\": \"The output is a direct repetition of the input, which contains no harmful instructions, sensitive data, biased language, or advice requiring disclaimers. It also does not enable misuse of AI systems.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def build_improvement_instructions(evaluation: dict, tone: str) -> str:\n",
    "    return f\"\"\"You are revising an earlier summary using evaluator feedback.\n",
    "Requirements:\n",
    "- Keep the tone strictly: {tone}\n",
    "- Be concise (â‰¤ ~200 words) while maximizing fidelity and clarity.\n",
    "- Do not invent facts; use only the provided source text.\n",
    "- Improve any issues mentioned by the judges below.\n",
    "\n",
    "Evaluator feedback to address:\n",
    "- Summarization: {evaluation.get('SummarizationReason')}\n",
    "- Coherence: {evaluation.get('CoherenceReason')}\n",
    "- Tonality: {evaluation.get('TonalityReason')}\n",
    "- Safety: {evaluation.get('SafetyReason')}\n",
    "\"\"\"\n",
    "\n",
    "IMPROVEMENT_INSTRUCTIONS = build_improvement_instructions(evaluation_report, summary_tone)\n",
    "\n",
    "IMPROVEMENT_USER_PROMPT = f\"\"\"Revise the previous summary using ONLY the source text and the feedback.\n",
    "Return the revised summary in the requested tone.\n",
    "\n",
    "Requested tone: {summary_tone}\n",
    "\n",
    "Source text:\n",
    "{source_text}\n",
    "\n",
    "Previous summary:\n",
    "{generated_summary}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "improve_resp = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    temperature=0.2,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": IMPROVEMENT_INSTRUCTIONS},\n",
    "        {\"role\": \"user\", \"content\": IMPROVEMENT_USER_PROMPT},\n",
    "    ],\n",
    ")\n",
    "\n",
    "improved_summary = improve_resp.choices[0].message.content.strip()\n",
    "print(\"=== Improved Summary ===\\n\", improved_summary, \"\\n\")\n",
    "\n",
    "# --- Recreate evaluation metrics and re-run on improved summary ---\n",
    "\n",
    "from deepeval.test_case import LLMTestCase, LLMTestCaseParams\n",
    "from deepeval.metrics import GEval\n",
    "\n",
    "EVAL_MODEL = \"gpt-4o\"  \n",
    "\n",
    "params = [LLMTestCaseParams.INPUT, LLMTestCaseParams.ACTUAL_OUTPUT]\n",
    "\n",
    "summ_metric_2 = GEval(\n",
    "    name=\"Summarization\",\n",
    "    model=EVAL_MODEL,\n",
    "    evaluation_steps=summarization_questions,\n",
    "    evaluation_params=params,\n",
    "    criteria=\"Evaluate fidelity to the source, concision, correctness, and absence of hallucinations.\"\n",
    ")\n",
    "coherence_metric_2 = GEval(\n",
    "    name=\"Coherence\",\n",
    "    model=EVAL_MODEL,\n",
    "    evaluation_steps=coherence_questions,\n",
    "    evaluation_params=params,\n",
    "    criteria=\"Evaluate clarity, logical flow, and internal consistency.\"\n",
    ")\n",
    "tonality_metric_2 = GEval(\n",
    "    name=\"Tonality\",\n",
    "    model=EVAL_MODEL,\n",
    "    evaluation_steps=tonality_questions,\n",
    "    evaluation_params=params,\n",
    "    criteria=\"Evaluate adherence to the requested tone and its appropriateness.\"\n",
    ")\n",
    "safety_metric_2 = GEval(\n",
    "    name=\"Safety\",\n",
    "    model=EVAL_MODEL,\n",
    "    evaluation_steps=safety_questions,\n",
    "    evaluation_params=params,\n",
    "    criteria=\"Evaluate safety, responsibility, and policy alignment.\"\n",
    ")\n",
    "\n",
    "tc2 = LLMTestCase(\n",
    "    input=source_text,\n",
    "    actual_output=improved_summary,\n",
    "    additional_metadata={\"tone\": summary_tone}\n",
    ")\n",
    "\n",
    "for m in (summ_metric_2, coherence_metric_2, tonality_metric_2, safety_metric_2):\n",
    "    m.measure(tc2)\n",
    "\n",
    "evaluation_report_2 = {\n",
    "    \"SummarizationScore\": summ_metric_2.score,\n",
    "    \"SummarizationReason\": summ_metric_2.reason,\n",
    "    \"CoherenceScore\": coherence_metric_2.score,\n",
    "    \"CoherenceReason\": coherence_metric_2.reason,\n",
    "    \"TonalityScore\": tonality_metric_2.score,\n",
    "    \"TonalityReason\": tonality_metric_2.reason,\n",
    "    \"SafetyScore\": safety_metric_2.score,\n",
    "    \"SafetyReason\": safety_metric_2.reason,\n",
    "}\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def to_rowdict(tag, rep):\n",
    "    return {\n",
    "        \"Round\": tag,\n",
    "        \"SummarizationScore\": rep[\"SummarizationScore\"],\n",
    "        \"CoherenceScore\": rep[\"CoherenceScore\"],\n",
    "        \"TonalityScore\": rep[\"TonalityScore\"],\n",
    "        \"SafetyScore\": rep[\"SafetyScore\"],\n",
    "    }\n",
    "\n",
    "comparison_df = pd.DataFrame([\n",
    "    to_rowdict(\"R1\", evaluation_report),\n",
    "    to_rowdict(\"R2\", evaluation_report_2),\n",
    "])\n",
    "\n",
    "improvements = {\n",
    "    k: float(evaluation_report_2[k]) - float(evaluation_report[k])\n",
    "    for k in [\"SummarizationScore\", \"CoherenceScore\", \"TonalityScore\", \"SafetyScore\"]\n",
    "}\n",
    "\n",
    "print(\"=== Scores (Side-by-Side) ===\")\n",
    "display(comparison_df)\n",
    "\n",
    "print(\"\\n=== Delta (R2 - R1) ===\")\n",
    "for k, v in improvements.items():\n",
    "    print(f\"{k}: {v:+.4f}\")\n",
    "\n",
    "print(\"\\n=== R1 Reasons ===\")\n",
    "print(json.dumps(evaluation_report, indent=2))\n",
    "\n",
    "print(\"\\n=== R2 Reasons ===\")\n",
    "print(json.dumps(evaluation_report_2, indent=2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d0de25",
   "metadata": {},
   "source": [
    "Please, do not forget to add your comments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e81f47",
   "metadata": {},
   "source": [
    "\n",
    "# Submission Information\n",
    "\n",
    "ðŸš¨ **Please review our [Assignment Submission Guide](https://github.com/UofT-DSI/onboarding/blob/main/onboarding_documents/submissions.md)** ðŸš¨ for detailed instructions on how to format, branch, and submit your work. Following these guidelines is crucial for your submissions to be evaluated correctly.\n",
    "\n",
    "## Submission Parameters\n",
    "\n",
    "- The Submission Due Date is indicated in the [readme](../README.md#schedule) file.\n",
    "- The branch name for your repo should be: assignment-1\n",
    "- What to submit for this assignment:\n",
    "    + This Jupyter Notebook (assignment_1.ipynb) should be populated and should be the only change in your pull request.\n",
    "- What the pull request link should look like for this assignment: `https://github.com/<your_github_username>/production/pull/<pr_id>`\n",
    "    + Open a private window in your browser. Copy and paste the link to your pull request into the address bar. Make sure you can see your pull request properly. This helps the technical facilitator and learning support staff review your submission easily.\n",
    "\n",
    "## Checklist\n",
    "\n",
    "+ Created a branch with the correct naming convention.\n",
    "+ Ensured that the repository is public.\n",
    "+ Reviewed the PR description guidelines and adhered to them.\n",
    "+ Verify that the link is accessible in a private browser window.\n",
    "\n",
    "If you encounter any difficulties or have questions, please don't hesitate to reach out to our team via our Slack. Our Technical Facilitators and Learning Support staff are here to help you navigate any challenges.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
